<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="摘星">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="摘星">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="摘星">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>摘星</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">摘星</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/20/store/tools/%E5%AE%B9%E6%98%93%E8%A2%AB%E8%AF%AF%E8%AF%BB%E7%9A%84IOSTAT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/20/store/tools/%E5%AE%B9%E6%98%93%E8%A2%AB%E8%AF%AF%E8%AF%BB%E7%9A%84IOSTAT/" class="post-title-link" itemprop="url">容易被误读的IOSTAT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-20 23:58:13" itemprop="dateCreated datePublished" datetime="2021-05-20T23:58:13+08:00">2021-05-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>iostat(1)是在Linux系统上查看I/O性能最基本的工具，然而对于那些熟悉其它UNIX系统的人来说它是很容易被误读的。<em>比如在HP-UX上 avserv（相当于Linux上的 svctm）是最重要的I/O指标，反映了硬盘设备的性能，它是指I/O请求从SCSI层发出、到I/O完成之后返回SCSI层所消耗的时间，不包括在SCSI队列中的等待时间，所以avserv体现了硬盘设备处理I/O的速度，又被称为disk service time，如果avserv很大，那么肯定是硬件出问题了。</em>然而Linux上svctm的含义截然不同，事实上在iostat(1)和sar(1)的man page上都说了不要相信svctm，该指标将被废弃：<br>“Warning! Do not trust this field any more. This field will be removed in a future sysstat version.”</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/20/store/tools/%E5%AE%B9%E6%98%93%E8%A2%AB%E8%AF%AF%E8%AF%BB%E7%9A%84IOSTAT/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/19/store/tools/%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90diskstats/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/19/store/tools/%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90diskstats/" class="post-title-link" itemprop="url">深入分析diskstats</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-19 23:58:13" itemprop="dateCreated datePublished" datetime="2021-05-19T23:58:13+08:00">2021-05-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>内核很多重要子系统均通过proc文件的方式，将自身的一些统计信息输出，方便最终用户查看各子系统的运行状态，这些统计信息被称为metrics,本文带你深入分析diskstats</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/19/store/tools/%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90diskstats/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/05/18/store/tools/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3iostat/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/18/store/tools/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3iostat/" class="post-title-link" itemprop="url">深入理解iostat</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-18 23:58:13" itemprop="dateCreated datePublished" datetime="2021-05-18T23:58:13+08:00">2021-05-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>iostat算是比较重要的查看块设备运行状态的工具，相信大多数使用Linux的同学都用过这个工具，或者听说过这个工具。但是对于这个工具，引起的误解也是最多的，大多数人对这个工具处于朦朦胧胧的状态。现在我们由浅到深地介绍这个工具，它输出的含义什么，介绍它的能力边界，介绍关于这个工具的常见误解。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/18/store/tools/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3iostat/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/24/store/NVMe/%E5%8D%81%E5%85%AB%E3%80%81SPDK%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/24/store/NVMe/%E5%8D%81%E5%85%AB%E3%80%81SPDK%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">十八、SPDK介绍</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-24 23:58:13" itemprop="dateCreated datePublished" datetime="2020-10-24T23:58:13+08:00">2020-10-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>固态存储设备正在取代数据中心。如今新一代的闪存存储，比起传统的磁盘设备，在性能、功耗和机架密度上具有显著的优势。这些现有的优势将会继续增大，使闪存作为下一代存储设备进入市场。 </p>
<p>用户使用现在的固态设备，比如 Intel® SSD DC P4600 Series Non-Volatile Memory Express（NVMe）驱动，面临一个主要的挑战：因为固态设备的吞吐量和延迟性能比起传统的磁盘好太多，如今总的处理时间中，存储软件占用了更大的比例。换句话说，存储软件栈的性能和效率在整个存储系统中越来越重要。随着存储设备继续发展，它将面临远远超过现今使用的软件体系结构的风险（即存储设备受制于相关软件的不足而不能发挥全部性能）同时，在接下来的几年里，存储设备将会继续飞快发展到令人难以置信的程度。 </p>
<p>为了提供一个完善的、端对端的参考存储体系结构—— Storage Performance Development Kit（SPDK）应运而生。SPDK已经证明很容易达到每秒钟数百万次I/O读取，通过使用多个处理器核心和多个NVMe SSD进行存储，而不需要额外的硬件。SPDK在BSD license许可协议下通过Github分发提供其全部的Linux参考架构的源代码。博客、邮件列表、文档和社区参与都可以在<a href="http://www.spdk.io中找到。" target="_blank" rel="noopener">http://www.spdk.io中找到。</a> </p>
<h1 id="软件体系结构概览"><a href="#软件体系结构概览" class="headerlink" title="软件体系结构概览"></a>软件体系结构概览</h1><p>SPDK如何工作？达到这样的超高性能运用了两个关键技术：运行于用户态和轮询模式。让我们进一步了解这两个软件工程术语。 </p>
<p>首先，我们的设备驱动代码运行在用户态。这意味着，根据定义，驱动代码不会在内核中运行。避免内核上下文切换和中断可以节省大量的处理开销，从而允许更多的时钟周期被用来做实际的数据存储。无论存储算法（去冗，加密，压缩，空白块存储）多么复杂，浪费更少的时钟周期意味着更好的性能和很低的延迟。这并不是说内核会增加不必要的开销；相反，内核增加了那些可能不适用于专用存储堆栈的通用计算用例相关的开销。</p>
<p>SPDK的指导原则是通过减少每一处额外的软件开销来达到最低时延和最高效率。 </p>
<p>其次，轮询模式驱动（Polled Mode Drivers, PMDs）改变了I/O的基本模型。在传统的I/O模型中，应用程序提交读写请求后睡眠，一旦I/O完成，中断就会将其唤醒。PMDs的工作方式则不同，应用程序提交读写请求后继续执行其他工作，以一定的时间间隔回过头来检查I/O是否已经完成。这种方式避免了中断带来的延迟和开销，并使得应用程序提高I/O的效率。在旋转设备时代（磁带和机械硬盘），中断开销只占整个I/O时间的很小的比例，因此给系统带来了巨大的效率提升。然而，在固态设备的时代，持续引入更低延迟的持久化设备，中断开销已然成为了整个I/O时间中不能被忽视的部分。这个问题在更低延迟的设备上只会越来越严重。系统已经能够每秒处理数百万个I/O，所以消除数百万个事务的这种开销，能够节省额外的CPU资源。 </p>
<p>SPDK由众多子组件组成，相互链接并共享用户态操作和轮询模式操作的共有部分。 每一个子组件的创建都是为了客户在构造端到端SPDK体系结构时遇到的特定功能和性能需求。同时，每一个子组件也可以被集成到非SPDK架构中，允许用户利用SPDK中用到的经验和技术来加速自己的软件。 </p>
<h1 id="主要的组件"><a href="#主要的组件" class="headerlink" title="主要的组件"></a>主要的组件</h1><h2 id="驱动-Drivers"><a href="#驱动-Drivers" class="headerlink" title="驱动(Drivers)"></a>驱动(Drivers)</h2><p>NVMe Driver：SPDK的基础组件，这个高优化无锁的驱动有着高扩展性、高效性和高性能的特点。 </p>
<p>Intel QuickData Technology：也称为Intel I/O Acceleration Technology（Inter IOAT，英特尔I/O加速技术），这是一种基于Xeon处理器平台上的copy offload引擎。通过提供用户空间访问，减少了DMA数据移动的阈值，允许对小尺寸I/O或NTB的更好利用。 </p>
<p>NVMe over Fabrics（NVMe-oF）initiator：从程序员的角度来看，本地SPDK NVMe驱动和NVMe-oF启动器共享一套共同的API命令。这意味着，例如本地/远程复制将十分容易实现。 </p>
<h2 id="存储服务-Storage-Services"><a href="#存储服务-Storage-Services" class="headerlink" title="存储服务(Storage Services)"></a>存储服务(Storage Services)</h2><p>Block device abstration layer（bdev）：这种通用的块设备抽象是连接到各种不同设备驱动和块设备的存储协议的粘合剂。并且还在块层中提供灵活的API，用于额外的用户功能，如磁盘阵列、压缩、去冗等等。 </p>
<p>Blobstore：为SPDK实现一个高精简的文件式语义（非POSIX）。这可以为数据库、容器、虚拟机或其他不依赖于大部分POSIX文件系统功能集（比如用户访问控制）的工作负载提供高性能基础。 </p>
<p>Blobstore Block Device：由SPDK Blobstore分配的块设备，是虚拟机或数据库可以与之交互的虚拟设备。这些设备得到SPDK基础架构的优势，意味着零拷贝和令人难以置信的可扩展性。 </p>
<p>Logical Volume：类似于内核软件栈中的逻辑卷管理，SPDK通过Blobstore的支持，同样带来了用户态逻辑卷的支持，包括更高级的按需分配、快照、克隆等功能。 </p>
<p>Ceph RADOS Block Device（RBD）：使Ceph成为SPDK的后端设备，比如这可能允许Ceph用作另一个存储层。 </p>
<p>Linux Asynchrounous I/O（AIO）：允许SPDK与内核设备（比如机械硬盘）交互。 </p>
<h2 id="存储协议-Storage-Protocols"><a href="#存储协议-Storage-Protocols" class="headerlink" title="存储协议(Storage Protocols)"></a>存储协议(Storage Protocols)</h2><p>iSCSI target：建立了通过以太网的块流量规范，大约是内核LIO效率的两倍。现在的版本默认使用内核TCP/IP协议栈，后期会加入对用户态TCP/IP协议栈的集成。 </p>
<p>NVMe-oF target：实现了NVMe-oF规范。将本地的高速设备通过网络暴露出来，结合SPDK通用块层和高效用户态驱动，实现跨网络环境下的丰富特性和高性能。支持的网络不限于RDMA一种，FC，TCP等作为Fabrics的不同实现，会陆续得到支持。 </p>
<p>vhost target：KVM/QEMU的功能利用了SPDK NVMe驱动，使得访客虚拟机访问存储设备时延迟更低，使得I/O密集型工作负载的整体CPU负载减低，支持不同的设备类型供虚拟机访问，比如SCSI, Block, NVMe块设备。 </p>
<h1 id="常见问题解答"><a href="#常见问题解答" class="headerlink" title="常见问题解答"></a>常见问题解答</h1><p>SPDK不适合于所有的存储体系结构。一些常见问题的解答也许会帮助用户判断SPDK组件是否适合自己的体系结构。 </p>
<p><strong>Q: SPDK是否基于Linux或FreeBSD？</strong> </p>
<p>A:SPDK主要在Linux上测试和支持。硬件驱动被FreeBSD和Linux支持。 </p>
<p><strong>Q: SPDK的硬件平台是否要求是Intel体系结构？</strong> </p>
<p>A: SPDK被设计为充分利用Intel平台的特性，并针对Intel芯片和系统进行测试和调整。 </p>
<p><strong>Q: SPDK的高性能路径是否运行在用户态？</strong> </p>
<p>A: SPDK通过更多地在用户态下运行从网卡到磁盘的高性能路径，提高性能和效率。通过将具有SPDK功能（比如NVMe-oF目标，NVMe-oF启动器，Blobstore）的应用程序结合起来，整个数据通路能够在用户空间运行，从而提供显著的高效率。 </p>
<p><strong>Q: SPDK的体系结构可以合并无锁的PMDs到它的线程模型吗？</strong> </p>
<p>A: 因为PMD持续运行在它们的线程中（而不是睡眠或者不用时让出处理器），所以它们有特殊的线程模型需求。 </p>
<p><strong>Q：SPDK现在是否使用DPDK处理网络数据包的工作负载？</strong> </p>
<p>A: SPDK和DPDK共享早期的编程模型，所以现在使用DPDK的用户可能会发现与SPDK紧密整合可以带来非常大的益处。同样地，正在使用SPDK的用户如果为网络处理添加DPDK功能，也可能会带来重大的改进。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/23/store/NVMe/%E5%8D%81%E4%B8%83%E3%80%81%E6%B5%85%E8%B0%88SPDK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/23/store/NVMe/%E5%8D%81%E4%B8%83%E3%80%81%E6%B5%85%E8%B0%88SPDK/" class="post-title-link" itemprop="url">十七、浅谈SPDK</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-23 23:58:13" itemprop="dateCreated datePublished" datetime="2020-10-23T23:58:13+08:00">2020-10-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>SPDK的全称是Storage Performance Development Kits，是Intel为了改善存储性能而推出的开发套件，针对的主要是Linux操作系统环境，当然了，开源，网址如下： </p>
<p><a href="http://www.spdk.io/" target="_blank" rel="noopener">http://www.spdk.io/</a> </p>
<p><a href="https://github.com/spdk/spdk" target="_blank" rel="noopener">https://github.com/spdk/spdk</a> </p>
<p>搞网络的朋友应该都知道DPDK，现在x86服务器平台上做高性能的网络包处理软件开发，几乎都会采用DPDK，而使用了DPDK的服务器上几乎一定都会指定需要使用Intel芯片的网卡。SPDK就是存储圈子里的DPDK，Intel推出SPDK的目的很简单，也是为了让大家更多的购买和使用Intel力推的NVMe接口SSD(包括使用3D Xpoint材料的Optane SSD)，并为将要推出的使用3D Xpoint的NVDIMM(ApachePass) 做准备。 </p>
<p>对于软件开发者来说，之所以需要使用SPDK，也正是因为新型的存储硬件性能进步太快，而传统存储软件堆栈的性能跟不上硬件发展的脚步，已经成为了瓶颈，所以就需要新的软件架构来进行革新。从下图存储系统中软硬件分别引入的延时比例比较就可以很明显的看到这个问题，简单来说，如果需要更高效率的使用NVMe接口的存储介质，你就需要考虑使用SPDK。 </p>
<p>SPDK解决性能问题的思路跟DPDK基本上是一脉相承，大致就是： </p>
<ol>
<li><p>Kernel Bypass 摒弃内核的低效处理方式，在用户态解决问题 </p>
</li>
<li><p>Pull Mode Driver 摒弃中断模式的驱动，采用更高效率的轮循模式驱动 </p>
</li>
<li><p>HugePage 使用大页表内存，避免内存拷贝造成的性能下降 </p>
</li>
<li><p>NUMA和IOAT，充分利用硬件特性来提升性能 </p>
</li>
<li><p>Lockless 使用各种无锁队列技术 </p>
</li>
</ol>
<p>而且SPDK的使用环境其实也依赖DPDK，因为有一些组件是共享使用的，也就是说想用SPDK，就先得装上DPDK </p>
<p>SPDK主要包含下列组件，这些组件的功能大致如下： </p>
<ul>
<li><p>NVMe driver：用户态的NVMe 驱动，相比内核态NVMe驱动大幅度提升性能 </p>
</li>
<li><p>I/OAT (DMA engine) driver：IOAT技术可以使用DMA引擎将数据在CPU和网卡的Cache之间直接进行拷贝提升性能 </p>
</li>
<li><p>NVMe over Fabrics target：开源的NVMe over Fabric Target实现，单核最高可以处理40Gbps的流量 </p>
</li>
<li><p>iSCSI target： 用户态的iSCSI target实现，相比Linux内核集成的LIO有约2倍的性能提升 </p>
</li>
<li><p>vhost target：提供本地虚拟机(KVM/QEMU)访问SPDK NVMe存储 </p>
</li>
</ul>
<p>在SPDK最新的版本里还增加了块设备抽象层和BlobFS这种伪文件系统的支持 </p>
<ul>
<li><p>Block Device Abstraction Layer ：在使用BDAL以后，SPDK可以通过扩展模块引入Intel的ISA-L库对存储进行各种处理，引入诸如类似RAID、去重、压缩、EraseCode以及加密等各种处理，也可以加入对NVDIMM的支持比如使用NVDIMM存放写日志等 </p>
</li>
<li><p>Blobstore </p>
</li>
<li><p>BlobFS (Blobstore Filesystem) </p>
</li>
</ul>
<p>在使用SPDK自带的用户态NVMe驱动时，根据Intel的说法，单核处理器就可以做到每秒处理360万IOPS性能，当然了，这360万IOPS性能需要用到8块P3700 NVMe SSD，毕竟单块P3700 SSD的最高性能也就只有46万。 </p>
<p>至于NVMe over Fabric，因为需要准备的测试环境要求比较高，很惭愧，我也还没有玩过，等以后有机会能测一下了再跟大家分享吧。 </p>
<p>您也可以直接在Intel官网上查看相关资料 </p>
<p><a href="https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk" target="_blank" rel="noopener">https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk</a> </p>
<p><a href="https://software.intel.com/en-us/articles/accelerating-your-nvme-drives-with-spdk" target="_blank" rel="noopener">https://software.intel.com/en-us/articles/accelerating-your-nvme-drives-with-spdk</a> </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/22/store/NVMe/%E5%8D%81%E5%85%AD%E3%80%81%E5%85%B3%E4%BA%8ENVMe%E5%92%8CNVMeoF%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/22/store/NVMe/%E5%8D%81%E5%85%AD%E3%80%81%E5%85%B3%E4%BA%8ENVMe%E5%92%8CNVMeoF%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/" class="post-title-link" itemprop="url">十六、关于NVMe和NVMeoF知识补充</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-22 23:58:13" itemprop="dateCreated datePublished" datetime="2020-10-22T23:58:13+08:00">2020-10-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>作为Nonvolatile Memory Express的缩写，NVMe是一种使固态硬盘（SSD）速度更快的协议，并且在企业用户中越来越受欢迎。 </p>
<p>了解NVMe的最简单方法可能是类比。想象一下，你刚买了一辆比普通汽车快15倍的跑车。您的新车不会超过100英里/小时，而是可以达到每小时1500英里。 </p>
<p>唯一的问题是，要往返于高速公路，您必须沿着25英里/小时的速度限制和保险杠到高速公路的车道下行。 </p>
<p>如果只是你不必在愚蠢的小街上浪费时间，你就会知道你的整个通勤可能会更快。你可能会开始寻找更快的替代方案。 </p>
<p>制造商推出基于闪存的固态硬盘后，这种情况有点像存储行业的情况。闪存技术比硬盘驱动器中的旋转磁盘快许多倍（速度快多少取决于您使用的设备），但早期驱动器使用与硬盘驱动器相同的SATA或SAS连接连接到系统和网络。虽然这些接口对于硬盘驱动器（HDD）可提供的性能类型来说已经足够了，但它们却为SSD造成了瓶颈。对于企业大数据分析和其他数据密集型工作负载而言，这尤其成问题。 </p>
<p><img src="/pics/image-20210418150858265.png" alt="image-20210418150858265"></p>
<h1 id="什么是NVMe？"><a href="#什么是NVMe？" class="headerlink" title="什么是NVMe？"></a>什么是NVMe？</h1><p>NVMe协议标准由NVM Express公司监管，NVM Express公司是一个由100多个组织组成的联盟，这些组织有兴趣开发更快的协议以提高非易失性存储器的性能。该组织由13名发起人组成，其中包括Cavium，思科，戴尔EMC，Facebook，英特尔，美光，Microsemi，微软，NetApp，三星，希捷，东芝内存和西部数据。 </p>
<p>NVMe的官方定义将其描述为：一系列标准和信息的开放式集合，以充分揭示从移动设备到数据中心的各种类型计算环境中非易失性存储器的优势.NVMe从一开始就设计为提供高带宽和低延迟当前和未来NVM技术的存储访问。 </p>
<p>NVMe是一种接口规范，用于通过PCI Express总线将存储连接到服务器。通俗地说，这是SSD与主机系统通信的更快捷方式。它有助于缓解当快速闪存通过最初为HDD设计的SAS或SATA连接连接到系统时发生的瓶颈。 </p>
<p><img src="/pics/image-20210418152020338.png" alt="image-20210418152020338"></p>
<p>英特尔750系列是采用NVMe技术的众多固态硬盘之一。 </p>
<h1 id="NVMe速度比SATA或SAS快得多"><a href="#NVMe速度比SATA或SAS快得多" class="headerlink" title="NVMe速度比SATA或SAS快得多"></a>NVMe速度比SATA或SAS快得多</h1><p>一个NVMe控制器在几个不同的方式加速性能。一种是使用PCIe总线，它将存储直接连接到系统CPU。这种直接连接消除了SATA所需的一些步骤，提高了整体性能。 </p>
<p>此外，NVMe SSD大规模实现并行性以显着提高吞吐量。当数据从存储器传输到服务器主机时，它会进入一行或一个队列。传统SATA连接只能支持一个队列，一次只能有32个数据串联。再回到那辆车的比喻，就像只有一条车道，可以在路上行驶32辆车。 </p>
<p>但NVMe存储支持多达64,000个队列，每个队列有64,000个条目。换句话说，就像从单车道公路到64,000车道公路，每条车道可容纳64,000辆车。如下图所示，这对整体性能产生巨大影响。 </p>
<p>还记得回到我们的汽车比喻和小巷25英里/小时的限速吗？SATA和SAS连接也会限制速度。对于SATA，理论最大传输速度为6.0 Gbps（实际上，最大传输速度要低得多）。这为SATA固态硬盘的速度提供了上限。超过某一点，使用更快的闪存对系统的整体性能没有影响，因为SATA连接会产生瓶颈。 </p>
<p>对于大多数消费者而言，SATA SSD为他们可能定期执行的任务提供了足够的性能。由于SATA SSD比NVMe SSD便宜，大多数消费者固态存储仍然使用SATA接口。 </p>
<p>但是，企业通常使用比消费者更多的数据，对于他们来说，SATA连接所带来的延迟可能成为一个问题。 </p>
<p>与SAS相比，NVMe提供了一种快速通过系统移动数据的方法，因此可用于当今最先进的系统。 </p>
<p><img src="/pics/image-20210418152054888.png" alt="image-20210418152054888"></p>
<h1 id="只有SSD在NVMe上运行？"><a href="#只有SSD在NVMe上运行？" class="headerlink" title="只有SSD在NVMe上运行？"></a>只有SSD在NVMe上运行？</h1><p>由于NVMe驱动器比SATA快得多，因此将NVMe与硬盘驱动器配合使用可能会提高性能。但请记住，NVMe代表Nonvolatile Memory Express，它专门设计用于非易失性存储器，如Nand闪存（尽管它也可以用于更新形式的非易失性存储器，如3D XPoint。） </p>
<p>当系统从旋转磁盘读取数据时，它一次只能读取一个数据。那是因为它必须旋转直到它到达第一段数据的正确物理位置，然后旋转并移动到第二段数据的正确位置，依此类推。 </p>
<p>另一方面，闪存和其他非易失性存储器技术没有移动部件。这意味着系统可以同时自由地从许多不同位置读取数据。这就是为什么SSD可以利用NVMe提供的并行性，但HDD不能。 </p>
<h1 id="PCIe和NVMe是相关的，但不是一回事"><a href="#PCIe和NVMe是相关的，但不是一回事" class="headerlink" title="PCIe和NVMe是相关的，但不是一回事"></a>PCIe和NVMe是相关的，但不是一回事</h1><p>对于许多人来说，NVMe最令人困惑的部分是它与PCIe的关系。一些供应商将他们的SSD称为NVMe标签，一些供应商使用PCIe标签，有些供应商似乎可以互换使用这些术语。 </p>
<p>虽然PCIe和NVMe密切相关，但这些术语指的是略有不同的技术。您可以将PCIe视为系统的物理部分。将NVMe SSD插入服务器时，将通过PCIe插槽连接。 </p>
<p>相比之下，NVMe是一种协议，一组允许SSD使用PCIe总线的硬件和软件标准。从外行的角度来看，您可以将NVMe视为允许存储设备与服务器连接的语言，而PCIe是实际的物理连接。 </p>
<h1 id="NVMe-over-Fabric将SSD连接到网络"><a href="#NVMe-over-Fabric将SSD连接到网络" class="headerlink" title="NVMe over Fabric将SSD连接到网络"></a>NVMe over Fabric将SSD连接到网络</h1><p>本文的大部分内容都集中在将SSD直接连接到服务器的标准NVMe上，但NVM Express组织还发布了针对NVMe over Fabric（NVMe-oF）的规范，该规范将用于块存储的非易失性存储器连接到网络。据该组称：NVM Express over Fabrics定义了一种通用架构，支持通过存储网络结构为NVMe块存储协议提供一系列存储网络结构。这包括为存储系统启用前端接口，扩展到大量NVMe设备和扩展数据中心内的距离，可以访问NVMe设备和NVMe子系统。 </p>
<p>大约90％的NVME-oF规范与NVMe规范相同; 但是，它确实使用了不同的传输映射机制。NVME-oF还有两种不同的变体：一种用于RDMA，另一种用于光纤通道。 </p>
<p>NVMe-oF规范也比NVMe规范要新得多，虽然一些厂商宣布支持该技术，但实际上很少有人销售NVME-oF产品。寻求这项技术在未来几年变得更加普及。 </p>
<table>
<thead>
<tr>
<th>SATA / SAS / PCIe / NVMe时间线</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>2003</td>
<td>SATA 1.0发布</td>
</tr>
<tr>
<td>2003</td>
<td>PCIe 1.0发布</td>
</tr>
<tr>
<td>2004年</td>
<td>SATA 2.0发布</td>
</tr>
<tr>
<td>2004年</td>
<td>SAS 1.0发布</td>
</tr>
<tr>
<td>2007年</td>
<td>PCIe 2.0发布</td>
</tr>
<tr>
<td>2009年</td>
<td>SAS 2.0发布</td>
</tr>
<tr>
<td>2009年</td>
<td>SATA 3.0发布</td>
</tr>
<tr>
<td>2009年</td>
<td>开始使用NVMe标准</td>
</tr>
<tr>
<td>2010</td>
<td>PCIe 3.0发布</td>
</tr>
<tr>
<td>2011</td>
<td>NVMe 1.0发布</td>
</tr>
</tbody></table>
<p>毫无疑问，NVMe速度令人印象深刻：存储和服务器供应商的蓝带联盟开发了NVMe作为高性能接口规范，使用PCIe总线加速NAND SSD。 </p>
<p>NVMe的逻辑设备接口利用SSD的低延迟和并行性来改善IOP和吞吐量并减少延迟。这不是供应商第一次使用PCIe来加速SSD，但这是第一种标准化方法。 </p>
<p>NVMe代表非易失性存储器，并且协议通过闪存特定的改进提供高带宽和低延迟。它支持当前的NAND闪存，并将扩展以支持依赖于持久存储器技术的未来高性能设备。 </p>
<h1 id="NVMe速度-NVMe-AHCI-SATA"><a href="#NVMe速度-NVMe-AHCI-SATA" class="headerlink" title="NVMe速度(NVMe/AHCI/SATA)"></a>NVMe速度(NVMe/AHCI/SATA)</h1><p>NVMe的性能和吞吐量有多快？显然足以引起数据存储行业的关注。 </p>
<ul>
<li><p>高达64K的队列： NVMe专为提高速度而设计，使用PCIe通过共享内存映射操作，简化内部软件，并优化多达64,000个队列的I / O. </p>
</li>
<li><p>加速其他顶级格式： NVMe的功能使其显着快于传统SAS和SATA SSD协议，更不用说SAS / SATA HDD。 </p>
</li>
<li><p>最高速率最快的NVMe驱动器通常只能通过OEM或大型企业客户使用，读取速度为3 GB / s，写入速度为1 GB / s。相同的驱动器可提供300,000多个随机读取IOP和40,000-50,000个写入IOP。 </p>
</li>
<li><p>SSD传输速率：中型数据中心的最佳NVMe驱动器无法达到这些平流层速度，但也更便宜。例如，三星的983 DCT NVMe驱动器具有1.92 TB的容量。该驱动器实现了1900 MB / s的连续写入速度，540k IOP的随机读取以及50k IOP的随机写入。 </p>
</li>
</ul>
<p><img src="/pics/image-20210418152300800.png" alt="image-20210418152300800"></p>
<p>在NVMe与SATA速度的比较中，SATA确实比NVMe有一些优势。它被广泛部署，其SSD速度足以满足许多应用的需求。虽然客户可能希望在技术更新期间升级到SAS或NVMe，但无需立即翻录和更换SATA。 </p>
<p>供应商仍在为SATA开发。三星开发了消费级SATA固态硬盘接口，测试读取速度高达3500 MB / s，写入速度高达2500 MB / s。虽然客户不会将这些设备添加到数据中心，但它们将加速消费者设备。 </p>
<p>SATA不能很好地处理高事务应用程序的高性能级别。对于这些，NVMe性能是您更好的选择。如果您的存储平台提供需要NVMe高SSD传输速率的高需求应用程序，那么这是非常值得的。 </p>
<h1 id="NVMe速度背后的架构"><a href="#NVMe速度背后的架构" class="headerlink" title="NVMe速度背后的架构"></a>NVMe速度背后的架构</h1><p>NVMe SSD以32 GBps（每秒千兆字节）的速率显示吞吐量。50万IOP是常见的，高端驱动器的IOP高达1000万。尽管有这些高速度，但延迟率通常保持在20微秒以下，有些只有一半。按照传统标准，这些数字令人印象深刻。 </p>
<h1 id="NVMe接口外形因素"><a href="#NVMe接口外形因素" class="headerlink" title="NVMe接口外形因素"></a>NVMe接口外形因素</h1><p>NVMe规范采用标准尺寸的PCIe扩展卡，或采用2.5英寸外形尺寸，四通道PCIe接口通过U.2连接器。U.2易于部署是最受欢迎的选择，它将SSD连接到主机，</p>
<p>可与PCIe，SAS或SATA配合使用。NVMe U.2驱动器通常具有四个PCIe通道，两个SAS通道和一个SATA通道，可在2.5英寸外形中支持广泛的接口。 </p>
<p>用于PCIe，SATA或USB外形尺寸的M.2迷你板规格也越来越受消费者级别NVMe使用的影响。M.2板有多种尺寸，包括最小的PCIe占用空间。NVMe磁盘上的存储容量从消费者大小的450GB开始，到数据中心的容量上升到11TB。 </p>
<h1 id="提升NVMe速度的常用选项"><a href="#提升NVMe速度的常用选项" class="headerlink" title="提升NVMe速度的常用选项"></a>提升NVMe速度的常用选项</h1><ul>
<li><p>SSD并行性。该架构利用SSD并行性来减少IO开销。HDD和磁带对访问模式很敏感。顺序数据可以提高性能，而随机数据可以减慢数据访问速度。SSD并行运行，因此随机或顺序数据对SSD性能影响不大。 </p>
</li>
<li><p>更新了总线。阵列经常出现瓶颈，因为SSD层的速度比HDD存储接口所支持的速度更快。IT使用多核处理器和大量RAM进行补偿。但是，部署NVMe可以更有效，更便宜，它可以利用SSD速度。 </p>
</li>
<li><p>性能增强。其他增强功能包括支持64K的单个队列容量，以及同时处理大约64K这些长队列的能力。随着延迟减少，这加速了处理同时请求的繁忙服务器的性能。 </p>
</li>
<li><p>RDMA。NVMe使用PCIe总线采用远程直接存储器访问（RDMA）。这使接口能够将IO命令和响应映射到主机共享内存，从而释放CPU资源。NVMe还简化了其命令集，发出不到一半的CPU指令作为SATA或SAS。（需要10个管理命令，5个是可选的;需要3个IO命令，8个是可选的。） </p>
</li>
<li><p>其他高级功能。NVMe支持安全容器命令，电源管理和命令增强等功能。主机内存缓冲区有助于支持客户端和移动NVMe。 </p>
</li>
<li><p>控制器内存缓冲区。NVMe缓冲区使主机能够在控制器存储器中制定命令，而不是依赖于通过PCIe的获取命令。NVMe传递内存块而不是SCSI命令，这样可以降低延迟。NVMe还通过观察服务级别协议参数来仲裁优先级命令。 </p>
</li>
<li><p>预订。NVMe支持Windows群集中的多主机预留，通过管理共享命名空间来协调主机访问。 </p>
</li>
</ul>
<p><img src="/pics/image-20210418152404133.png" alt="image-20210418152404133"></p>
<h1 id="关于NVMe-oF（Fabric上的NVMe）"><a href="#关于NVMe-oF（Fabric上的NVMe）" class="headerlink" title="关于NVMe-oF（Fabric上的NVMe）"></a>关于NVMe-oF（Fabric上的NVMe）</h1><p>但是，NVMe的所有性能优势都取决于对单个主机的直接连接。存储控制器是共享存储的必需品，并且管理容量供应，一些数据保护，物理寻址和协议转换。但是添加一个存储控制器并且NVMe速度变慢，从而失去了高成本的理由。 </p>
<p>NVMe over Fabrics（NVMe-oF）是一种解决问题的方法。该规范使基于NVMe消息的命令能够通过以太网，光纤通道（FC）或InfiniBand结构传输数据，而无需通过慢速存储控制器。存储管理员可以将NVMe SSD从服务器中取出并通过结构连接。远程SSD存储服务以极低的延迟以内存到内存传输的速度运行。 </p>
<p>该规范使用RDMA进行InfiniBand，融合以太网和Internet广域。它使用第二种方法跨光纤通道传输。 </p>
<h1 id="NVMe瓶颈与局限"><a href="#NVMe瓶颈与局限" class="headerlink" title="NVMe瓶颈与局限"></a>NVMe瓶颈与局限</h1><p>没有什么是完美的，包括NVMe速度。 </p>
<ul>
<li><p>热量节流。这是可能的M.2固态硬盘的PCIe产生热量与持续使用，使得节流必要的。然而，垂直mM2形状因子通风更好并解决了问题。实际上，这些驱动器的运行速度非常快，通常在热量成为问题之前完成传输。 </p>
</li>
<li><p>NAND SSD的速度。NVMe发布了SSD速度和旧存储接口之间的瓶颈，因此速度瓶颈主要出现在NAND闪存中。该技术在不断改进，市场预计SSD速度会有更多性能提升。NVMe随时准备为他们提供支持。 </p>
</li>
<li><p>SSD-SSD传输速度慢。将数据SATA或SAS SSD传输到NVMe SSD时，数据移动速度会降低。系统无法在NVMe SSD上更快地写入数据，而不是较慢的接口可以提供的数据。 </p>
</li>
<li><p>缺乏高级存储功能。全闪存阵列一直在改进其高级存储功能，包括加密，重复数据删除和压缩，复制和快照。虽然NVMe驱动器制造商通常会添加保护驱动器端到端数据完整性的功能，但较新的NVMe驱动器还缺乏此级别的功能。 </p>
</li>
<li><p>价格。虽然SSD价格正在下降，但NVMe价格昂贵且对每个存储环境都没有必要。它的主要用途是支持高事务数据库和业务关键应用程序的密集性能。如果IT将NVMe用于性能需求较低的其他应用程序，那么它们将是浪费预算和IO带宽。通过在分层存储架构中将它们与SATA或SAS闪存配对，并将老化数据迁移到HDD，磁带或云，IT可以节省资金。 </p>
</li>
<li><p>关于NVMe成本的另一个注意事项。对于大多数数据中心而言，中级NVMe驱动器将绰绰有余。很少有数据中心处理如此多的数据，甚至还需要顶级NVMe的极高性能和吞吐速度。保守购买，这样您就不会在非活动驱动器上超支。还要确保NVMe驱动器的容量足以满足您的存储需求。 </p>
</li>
</ul>
<h1 id="NVMe高性能应用场景"><a href="#NVMe高性能应用场景" class="headerlink" title="NVMe高性能应用场景"></a>NVMe高性能应用场景</h1><p>生命科学，金融服务和能源公司都依赖于极快的HPC，具有高性能和低延迟。生命科学和能源利用NVMe速度进行快速复杂的计算，因为它实际上消除了从存储读取时的处理器等待时间。金融服务使用NVMe作为辅助内存来加速极高数量的交易。在生命科学领域，行业测试显示出比SATA更高的性能提升6倍。该行业为此付出了代价 - 大约50％的价格上涨 - 但总投资回报率非常有利。 </p>
<p>OLTP关系数据库和大数据也受益于高性能读取。在数据库中，DBA可以使用SSD缓存来固定元数据，数据和索引，而不会降低速度。查询速度大大提高，大大提高了数据库性能。大数据密集型工作负载不再遇到存储瓶颈，这使业务分析师可以使用即时可用数据做出实时决策。由于NVMe不仅限于特定类型的工作负载，因此可以加速其他应用程序的性能。 </p>
<p>在一个鲜为人知的用例中，NVMe允许管理员通过增加虚拟化网络可以支持的虚拟机数量来优化虚拟化环境。VMware和Hyper-V管理员经常通过按工作负载，延迟或IOP划分网络来优化VM性能。这增加了费用和管理的复杂性。NVMe专门用于管理群集并优化跨工作负载的性能。这使管理员可以提高网络速度和性能，并省去复杂的分区。这不是一个便宜的主张，但通过将NVMe应用于关键虚拟机 - 并密切关注价格下跌 - 管理员可以轻松证明NVMe的额外费用。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/21/store/NVMe/%E5%8D%81%E4%BA%94%E3%80%81%E5%9F%BA%E4%BA%8EFC%E7%9A%84NVMe%E6%88%96FC-NVMe%E6%A0%87%E5%87%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/21/store/NVMe/%E5%8D%81%E4%BA%94%E3%80%81%E5%9F%BA%E4%BA%8EFC%E7%9A%84NVMe%E6%88%96FC-NVMe%E6%A0%87%E5%87%86/" class="post-title-link" itemprop="url">十五、基于FC的NVMe或FC-NVMe标准</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-21 23:58:13" itemprop="dateCreated datePublished" datetime="2020-10-21T23:58:13+08:00">2020-10-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>光纤通道实现的NVMe(FC-NVMe标准实现)是一项技术规范，旨在实现在主机和光纤通道网络结构上的目标存储子系统上传输NVMe的消息命令和信息。 </p>
<p>光纤通道是面向NVMe over Fabrics(NVMe-oF)的Fabric传输选项，由NVM Express Inc.(一家拥有100多家成员技术公司的非营利组织)开发的规范。其他NVMe传输选项包括以太网和InfiniBand上的远程直接内存访问(RDMA)。NVM Express Inc.于2016年6月5日发布了1.0版NVMe-oF。 </p>
<p>国际信息技术标准委员会(INCITS)的T11委员会定义了一种帧格式和映射协议，将NVMe-oF应用到光纤通道。T11委员会于2017年8月完成了FC-NVMe标准的第一版，并将其提交给INCITS出版。 </p>
<h1 id="FC-NVMe如何工作"><a href="#FC-NVMe如何工作" class="headerlink" title="FC NVMe如何工作"></a>FC NVMe如何工作</h1><p>FC协议(FCP)允许上层传输协议，如NVMe，小型计算机系统接口(SCSI)和IBM专有光纤连接(FICON)的映射，以实现主机和外围目标存储设备或系统之间的数据和命令传输。 </p>
<p>与SCSI和FICON相比，NVMe具有简化的寄存器接口和命令集，减少了输入/输出(I/O)堆栈的CPU开销，降低了延迟并提高了性能。NVM Express Inc.开发了适用于快速介质的NVMe，包括固态硬盘(SSD)和其他基于内存的</p>
<p>技术。相比之下，SCSI命令集是在较慢的硬盘驱动器(HDD)和磁带作为主要存储介质的时候设计的，而FICON则是为连接大型计算机和存储设备而设计的。 </p>
<p><img src="/pics/image-20210418144801040.png" alt="image-20210418144801040"></p>
<p>NVMe传输是一种抽象协议层，旨在提供可靠的NVMe命令和数据传输。 </p>
<p>FC-NVMe将NVMe命令集简化为基本的FCP指令。由于光纤通道专为存储流量而设计，因此系统中内置了诸如发现，管理和设备端到端验证等功能。 </p>
<p>NVMe-oF(包括通过光纤通道的NVMe)和NVMe之间的主要区别是传输命令的机制。NVMe通过外围组件互连Express(PCIe)接口协议将请求和响应映射到主机中的共享内存。NVMe-oF使用基于消息的模型通过网络在主机和目标存储设备之间发送请求和响应。 </p>
<p>NVMe-oF替代PCIe来扩展NVMe主机和NVMe存储子系统进行通信的距离。与使用本地主机的PCIe 总线的NVMe存储设备的延迟相比，NVMe-oF的最初设计目标是在通过合适的网络结构连接的NVMe主机和NVMe存储目标之间添加不超过10 微秒的延迟。 </p>
<p>在大规模基于块闪存的存储环境最有可能采用NVMe over FC。FC-NVMe光纤通道提供的NVMe-oF结构、可预测性和可靠性特性等与给SCSI提供的相同，另外，NVMe-oF流量和传统的基于SCSI的流量可以在同一FC结构上同时运行。 </p>
<p><img src="/pics/image-20210418144920587.png" alt="image-20210418144920587"></p>
<p>基于FC标准的NVMe定义了FC-NVMe协议层。NVMe over Fabrics规范定义了NVMe-oF协议层。NVMe规范定义了NVMe主机软件和NVM子系统协议层。 </p>
<p>要求必须支持基于光纤通道的NVMe才能发挥潜在优势的基础架构组件，包括存储操作系统(OS)和网络适配器卡。FC存储系统供应商必须让其产品符合FC-NVMe的要求。目前支持FC-NVMe的主机总线适配器(HBA)的供应商包括Broadcom和Cavium。Broadcom和思科是主要的FC交换机供应商。 </p>
<h1 id="FC-NVMe的优点和缺点"><a href="#FC-NVMe的优点和缺点" class="headerlink" title="FC-NVMe的优点和缺点"></a>FC-NVMe的优点和缺点</h1><p>与HDD或串行高级技术的SCSI命令集(SATA或串行SCSI SAS SSD)进行数据传输相比，FC-NVMe具有更高的性能，更低的延迟。基于NVMe的SSD的一个缺点可能是成本较高，但NVMe SSD的价格有望与某些类型的传统SSD达成平衡。 </p>
<p>将FC-NVMe与基于以太网或InfiniBand的NVMe-oF替代方案进行比较，如果考虑网络技术的优缺点，光纤通道结构以其无损数据传输，可预测和一致的性能以及可靠性而闻名。大型企业倾向于将FC存储用于关键任务工作负载。但光纤通道需要特殊的设备和存储网络专业知识才能运行，并且可能比基于以太网的替代方案更昂贵。 </p>
<p>基于以太网的NVMe存储产品往往比基于FC-NVMe的选件更丰富。大多数存储创业公司都专注于基于以太网的NVMe，并且有时采用专有技术来更快地将其产品推向市场。 </p>
<p>基于InfiniBand的NVMe倾向于吸引需要极高带宽和低延迟的高性能计算工作负载。InfiniBand网络通常用于后端存储系统内的通信，而不是主机到存储器的通信。与FC一样，InfiniBand是一个需要特殊硬件的无损网络，它具有诸如流量和拥塞控制以及服务质量(QoS)等优点。但与FC不同的是，InfiniBand和以太网缺少发现服务自动将节点添加到结构中。 </p>
<p>NVMe-oF规范支持RDMA(但并非必需)，映射方式包括用于以太网和InfiniBand的融合以太网(RoCE)上的RDMA和用于互联网的广域RDMA协议(iWARP)。NVMe Express组织还计划支持传输控制协议(TCP)的传输选项 </p>
<p>Brocade最近发表了对NVMe over Fabric理解和观点，认为FC Fabric相比以太网具有很多优势，并且FC聚焦数据中心数据传输和交换，具有更好的网络安全性。 </p>
<p>目前，基于SCSI的全闪存和混合阵列正成为数据中心的主流，但与此同时，一种为固态PCIe模块专门构建的非易失性存储器(NVMe)标准已经成为服务</p>
<p>器连接Flash的一个新的高性能接口。NVMe通过低延迟和增强队列机制提供了更好的随机和连续性能，并增加了传统协议(如SAS)应用程序的并行性。 </p>
<p>为了支持数据中心的网络存储，通过NVMe over Fabric实现NVMe标准在PCIe总线上的扩展，以此来挑战SCSI在SAN中的统治地位。NV Me over Fabric支持把NVMe映射到多个Fabrics传输选项，主要包括FC、InfiniBand、RoCE v2和iWARP。 </p>
<p>随着NVMe的发展和NVMe over Fabric技术商品化，在SAN市场，将NVMe定位为SCSI的替代方案，这也为Flash模块供应商打开一扇门来应对这个新的市场。自然地，存储市场的新来者试图吹捧他们的技术有优势，然而，新技术的缺点可能没有受到太多关注。通过作者的分析，希望大家能对NVMe和NVMe over Fabrics技术有个综合全面的认识。 </p>
<h2 id="FC不但可以作为NVMe的Fabrics且更有优势"><a href="#FC不但可以作为NVMe的Fabrics且更有优势" class="headerlink" title="FC不但可以作为NVMe的Fabrics且更有优势"></a>FC不但可以作为NVMe的Fabrics且更有优势</h2><p>FC实际上是支持NVMe的一种fabrics选择。NVMe over fabric白皮书上概述了对NVMe支持的两种类型的fabrics，一个是RDMA和一个是使用FC。尽管一些竞争者会声称光纤通道不是合法的NVMe Fabric，但是NVM Express白皮书例证说明了这个问题。 </p>
<p>同样，白皮书明确列出了光纤通道作为一个NVMe over Fabrics选择，也描述了理想的Fabrics需要具备可靠的、以Credit为基础的流量控制和交付机制。然而，基于Credit的流程控制机制是FC、InfiniBand和PCIe传输原生能力。基于Credit流控制不是以太网/ IP网络的一部分，所以，相比iWARP或RoCE的以太网Fabrics，FC实际上是NVMe更好的Fabrics选择。 </p>
<h2 id="RDMA也不是NVMe-Fabric的关键"><a href="#RDMA也不是NVMe-Fabric的关键" class="headerlink" title="RDMA也不是NVMe Fabric的关键"></a>RDMA也不是NVMe Fabric的关键</h2><p>RDMA提倡者一般声称RDMA对设计好NVMe Fabric很重要。但在NVMe的白皮书中并没有把RDMA列为“理想”NVMe over Fabric的重要属性，也就是说RDMA除了只是一种实现NVMe Fabric的方法外，没有什么特别的。在博科看来，InfiniBand社区在RDMA有较大投入且与PCIe社区合作紧密，但是NVMe和NVMe over Fabric本身并不依赖于RDMA。 </p>
<h2 id="SCSI也不是唯一的FC-Native协议"><a href="#SCSI也不是唯一的FC-Native协议" class="headerlink" title="SCSI也不是唯一的FC Native协议"></a>SCSI也不是唯一的FC Native协议</h2><p>RDMA倡导者通常将NVMe over以太网/IP和FC的延迟时间进行比较(这就像比较把IP和以太网比较一样)，由于NVMe是上层协议，光纤通道是链路层协议。完整的比较应该是把NVMe over以太网和SCSI over FC进行比较，如果描述正确的话，这才是一个有效的比较。现在，作为光纤通道专家也意识到一个问题，由于FC上承载SCSI叫光纤通道协议(FCP)，所以不止一个新手错误地认为所有的FC通信都必须是FCP。但事实上FCP与FC不一样，FCP仅仅是一种FC-4(上层)协议，类似于FICON(大型机存储协议)，可以通过FC传输。 </p>
<p>常常产生的一个误解是NVMe首先被翻译成底层SCSI(FCP)之后才运行在FC上。这种误解很有可能是由同样的白皮书引起的，它告诉我们理想的NVMe传输应该允许客户端“直接发送和接收本地NVMe命令，无需使用诸SCSI如此类的转换层”。这句话本身这是有道理的，因为NVMe是延迟优化的，而转换层却会引入延迟。 </p>
<p>实际上FC本身就可以运输NVMe，无需翻译和转化。NVMe over FC定义了一个新的上层FC-NVMe流量类型，它识别了特定于NVMe的帧。 </p>
<p>FC-NVMe标准组织认为在FC上同时支持NVMe和SCSI会具有更大价值。FC-NVMe标准规定了NVMe over FC使用与FCP相同IO框架类型。FC作为多协议结构的长期使用和应用表明FC SAN同时支持SCSI和NVMe是非常可靠的。 </p>
<h2 id="如何看待SCSI到NVMe转换层对NVMe产生的影响？"><a href="#如何看待SCSI到NVMe转换层对NVMe产生的影响？" class="headerlink" title="如何看待SCSI到NVMe转换层对NVMe产生的影响？"></a>如何看待SCSI到NVMe转换层对NVMe产生的影响？</h2><p>NVMe fabric聚焦于最低延迟，NVMe over fabric的白皮书说明传输的一个理想方式是不需要翻译层，如果存在SCSI到NVMe转换就是次优的传输方式。在编写应用程序时，如果能直接使用NVMe，不但有效避免翻译步骤，还将避免了每IO引入的时钟周期。FC不需要转换翻译且支持Native NVMe。 </p>
<p>与此同时，NVMe社区也意识到SCSI应用程序部署时，上层应用程序适配、兼容和SCSI到NVMe转换层的重要性。许多NVMe的潜在用户无法重新设计他们运行的应用程序，但希望能选择搬到NVMe基础设施之上，不依赖于它们的应用程序供应商重新设计。从这个角度来看，有一个转化翻译层作为一种选择对NVMe的采用和普及实际上是有益的。 </p>
<p>值得庆幸的是，目前业界主流的HBA厂商都提供了从SCSI到 NVMe转换翻译的驱动程序，同时也提供Native NVMe能力支持原生支持NVMe over Fabric应用程序。 </p>
<h2 id="FC能否实现零拷贝-Zero-Copy-功能？"><a href="#FC能否实现零拷贝-Zero-Copy-功能？" class="headerlink" title="FC能否实现零拷贝(Zero Copy)功能？"></a>FC能否实现零拷贝(Zero Copy)功能？</h2><p>IP堆栈当时被开发时，主要设计用于处理许多上层协议和许多层2网络，从令牌环到电话线，清晰的网络层划分对于互操作性有很好的意义，为了达到这个目的最好的选择是使用中间缓冲，使缓冲区复制公共数据。 </p>
<p>在20世纪80年代早期，提供单副本复制(Single-Copy)也算是一个好的网络协议栈，网络接口卡(NIC)接收帧后，通过DMA技术将它们写到与网络堆栈相关联的DRAM缓冲区中，然后堆栈会决定哪个应用应该接受、继续处理帧数据，并把数据复制到应用对应的DRAM缓存区中。 </p>
<p><img src="/pics/image-20210418145910351.png" alt="image-20210418145910351"></p>
<p>在20世纪80年代中期，随着FC的生产，一切发生了变化。FC主要特点就是速度，所以为了达到优化的目前，FC允许芯片技术更复杂，FC/SCSI堆栈的层数更少，也放开了IP堆栈所面临的向后兼容性的限制。因此，FC实现一个适配器、驱动模型的堆栈架构，从而消除单一副本(Single-Copy)。 </p>
<p><img src="/pics/image-20210418150005187.png" alt="image-20210418150005187"></p>
<p>事实也确实如此，当应用程序请求存储IO时，应用程序以“逻辑地址范围”的形式指定一个缓冲区地址，然后将其转换为DMA的物理地址范围实现DMA传输。有时，一个逻辑范围将映射到多个物理块，因此HBA采用Scatter-Gather List (SGL)完成数据传输和保存。FC通过提供零拷贝(Zero-Copy)技术，支持DMA数据传输。RDMA通过从本地服务器传递Scatter-Gather List到远程服务器有效地将本地内存与远程服务器共享，使远程服务器可以直接读取或写入本地服务器的内存。 </p>
<h2 id="IP上的零拷贝-Zero-Copy-也不需要RDMA"><a href="#IP上的零拷贝-Zero-Copy-也不需要RDMA" class="headerlink" title="IP上的零拷贝(Zero-Copy)也不需要RDMA"></a>IP上的零拷贝(Zero-Copy)也不需要RDMA</h2><p>由于RDMA越来越流行，因此在2007年将其扩展了到Internet Wide Area网络，从而形成RDMA协议(iWARP)标准。iWARP是建立在TCP之上的，传输协议使用确认和重传机制。TCP还采用一个“窗口”算法以避免传输超过了发送方和接收方之间的网络容量。 </p>
<p>在Internet Engineering Task Force (IETF) Requests For Comment (RFCs 5040–5044)中，前一个RFC 5040中描述了RDMA如何使用Direct Data Placement (DDP)协议来实现FC和InfiniBand的零拷贝(Zero-Copy)效率，后一个RFC 5044标记了TCP中PDU对齐规范，有效地禁用了TCP 的“合并”行为，使得NIC更容易地处理接收的数据，提供DDP的硬件支持。 </p>
<p><img src="/pics/image-20210418150102852.png" alt="image-20210418150102852"></p>
<p>前面提到的RFCs为零拷贝(Zero-Copy)效率提供了基础，但是传统的NICs没有TCP处理功能。软件实现虽然提供了互操作性，但无法满足RDMA性能要求。为此，新的称为TCP Offload Engines (TOEs)的NICs卡就产生了，然而早期的TOEs都不适合iWARP，只有基于硬件实现DDP能力的RDMA使能TOEs才能提供类似FC一样的零拷贝(Zero-Copy)效果。 </p>
<p>2009年前后，随着当时InfiniBand市场的低迷，NVMe获得了越来越多的关注，IETF的Transparent Interconnection of Lots of Links (TRILL) 和IEEE的 Data Center Bridging (DCB)获得发展动力并以太网成为无损的Fabric。其中TRILL是指除IEEE的生成树协议支持以外的任何以太网拓扑结构；DCB采用基于优先级的流量控制、增强的传输选择和数据中心桥接交换技术。 </p>
<p>InfiniBand行业协会(IBTA)看到了一个机会，在新的技术领域利用其在RDMA方面的专业知识，因此，他们开发了RDMA over Converged Ethernet (RoCE)规范(Converged Ethernet就是早期的DCB)。就像iWARP需要专门的TOEs来实现零拷贝(Zero-Copy)效率一样，RoCE依赖于RDMA-enabled NICs (RNICs)实现这一能力。IBTA认为RoCE的性能比iWARP更高，并指出了TCP (iWARP)不是低延迟通信的理想协议。 </p>
<p>因为以太网不提供类似TCP的可靠传输能力，RoCE标准是在更高层的协议堆栈中实现可靠性功能。在RoCE发布的时候，对IPv4地址有相关约束，对TRILL的2层以太网网络扩展能力也有很高要求。IBTA显然认为RoCE应该拥有交付大规模高性能RDMA所需的一切能力。 </p>
<h2 id="可路由的RoCE-v2才是更好的RoCE"><a href="#可路由的RoCE-v2才是更好的RoCE" class="headerlink" title="可路由的RoCE v2才是更好的RoCE"></a>可路由的RoCE v2才是更好的RoCE</h2><p>Hyper-Scale和软件定义的网络推崇者促使IBTA创建RoCE v2(有时会被称为“可路由的RoCE”)，意在取代RoCE。不同于基于TCP的iWARP， RoCE v2运行在UDP之上没有缓慢启动的节流行为。当然，采用UDP意味着RoCEv2帧不兼容RoCEv1帧(尽管支持RoCEv2的RDMA-enable的NICs通常可以配置为使用RoCE v1格式)。因为基于UDP的 RoCEv2缺乏类似TCP的显式拥塞通知能力，所以IBTA指出通过支持IETF的ECN实现在UDP之上传输层的流控制。 </p>
<h2 id="网络化的RDMA经历了一个艰难的阶段"><a href="#网络化的RDMA经历了一个艰难的阶段" class="headerlink" title="网络化的RDMA经历了一个艰难的阶段"></a>网络化的RDMA经历了一个艰难的阶段</h2><p>大约在2009年，NVMe获得了越来越多的关注，而当时InfiniBand市场的steam正处于低谷，IETF的大量链路透明互连(TRILL)和IEEE的数据中心桥接(DCB)作为让以太网成为无损结构的一种方式也获得了越来越多的关注。(创建TRILL是为了支持IEEE的生成树协议不支持的任何到任何以太网拓扑。DCB集成了基于优先级的流量控制、增强的传输选择和数据中心桥接交换。 </p>
<p>InfiniBand Trade Association (IBTA)看到了一个机会，可以在一个新的技术领域改变自己在RDMA方面的专长，因此他们通过聚合以太网(RoCE，发音为“rocky”)规范(“聚合以太网”是DCB的早期术语)开发了RDMA。正如iWARP需要特殊的TOE来提供零拷贝效率一样，RoCE依赖于rdma支持的NICs (RNICs)来实现这种性能。 </p>
<p>IBTA的RoCE宣称比iWARP更高的性能,指出TCP、iWARP的基础并不是理想的低延迟通信协议，当发起一个连接或者连接已经闲置了一段时间，在一定程度上TCP存在“慢启动”行为。由于以太网不提供TCP的可靠传输能力，RoCE标准在堆栈的更高层实现了这种能力。在RoCE发布的时候，IPv4地址约束是最重要的，TRILL承诺从根本上扩展第二层以太网网络和IP子网的规模。IBTA显然认为RoCE拥有交付大规模、高性能RDMA所需的一切。 </p>
<p>相反，超大规模的玩家和软件定义的网络推广者将这一天带到了“第3层到机架顶部”，导致了机架大小的IP子网，促使IBTA创建了RoCEv2(有时被称为“可路由的RoCE”)。与基于tcp的iWARP不同，RoCEv2运行在UDP之上，它没有慢启动节流行为。转向UDP意味着RoCEv2帧与RoCEv1帧不兼容(尽管支持RoCEv2的rdma支持的NICs通常可以配置为使用RoCEv1格式)。由于UDP缺乏TCP支持IETF的显式拥塞通知(又名ECN, RFCs 3168, 4301, 6040)， IBTA指定RoCEv2也支持IETF的ECN，在UDP之上的IB传输层实现流控制。 </p>
<p>目前的情况是，iWARP和RoCEv2都在争夺基于以太网的NVMe面料市场的所有权 </p>
<p>对其他交通工具的合理批评。 </p>
<p>Non-Volatile Memory Express社区是在2007年英特尔开发者论坛之后兴起的，有兴趣在PCI Express上标准化flash模块的接口。由此产生的NVMe规范更倾向于存储语义而不是内存语义，这主要是因为flash技术面向块的特性。 </p>
<h2 id="NVMe优于SCSI和ATA"><a href="#NVMe优于SCSI和ATA" class="headerlink" title="NVMe优于SCSI和ATA"></a>NVMe优于SCSI和ATA</h2><p>在NVMe开发之前，包括基于闪存的直接连接固态硬盘(ssd)通常通过串行附加SCSI (SAS)或串行ATA进行连接，这两种方式都是并行磁盘接口的序列化版本，最初是在1980年代为dos时代的PC行业定义的。随着操作系统和应用程序的成熟，很少有人注意到这些遗留协议中的复杂性和延迟开销，因为磁盘驱动器的旋转延迟和寻道时间控制了任何磁盘I/O的总体延迟。flash生态系统的成熟改变了这个现状。从一开始，flash就为读取带来了巨大的延迟优势，特别是对于磁盘架构特别薄弱的随机访问读取。写缓存提供了隐藏对flash进行写操作的缓慢速度的能力，但是flash的写耐力挑战和有限的密度限制了它早期对专门领域的适用性。 </p>
<p>随着flash密度的增长和算法的出现，以减轻flash的写擦写数问题，flash逐渐成为一个完全可行的替代旋转磁盘。行业对磁盘友好型SCSI的依赖减弱了，</p>
<p>对高性能SSD协议的开放程度增加了，NVMe就是答案。 </p>
<h2 id="RDMA需要iWARP补丁"><a href="#RDMA需要iWARP补丁" class="headerlink" title="RDMA需要iWARP补丁"></a>RDMA需要iWARP补丁</h2><p>尽管RDMA对于动态共享内存服务器集群应用程序来说是一个强大的协议，但对于存储(尤其是对于小写入)来说，它并不是一个固有的高效协议。假设服务器需要向存储器写入1024字节。RDMA模型是让服务器向存储设备发送一条消息，说:“嘿，我想把1024字节写入卷。字节位于我的内存中，地址在附件的散集列表中。存储设备接收消息和相关的散集列表，然后转过头来向服务器的内存发出RDMA读请求，然后服务器发送1024字节。 </p>
<p>这种传输方式需要三条消息。为了有助于其性能提升，NVMe社区认识到了这一弱点，并对RDMA原则闭上眼睛，针对NVMe定义了一种数据“封装”机制，用于同一个消息中发送有效负载数据。 </p>
<p>但这种“封装”机制与主流RDMA实现iWARP不兼容，所以iWARP社区立即对此进行了研究，在2014年IETF发布了RFC 7306对其进行描述。 </p>
<h2 id="复杂的协议栈并不理想"><a href="#复杂的协议栈并不理想" class="headerlink" title="复杂的协议栈并不理想"></a>复杂的协议栈并不理想</h2><p>NVMe协议比SCSI协议更高效的一个原因是NVMe的协议栈非常简单。看看不同NVMe结构的协议栈。Fibre Channel, RoCEv2, and iWARP协议栈如下。 </p>
<p><img src="/pics/image-20210418150248531.png" alt="image-20210418150248531"></p>
<p>IP/以太网相对于光纤通道的复杂性既不是随机的，也不是没有必要的。随着时间的推移，协议中有几个关键的差异导致了这种复杂性: </p>
<ul>
<li><p>以太网和IP(和TCP/UDP)实现传输层比光纤通道更独立。以太网/IP网络必须支持的全球范围内的地址分配和路由的挑战，需要多个复杂的层和算法。Fibre Channel是为数据中心规模设计的，这个问题本身很复杂，但比以太网/IP简单多了。 </p>
</li>
<li><p>以太网在网络的早期作为一种最佳的共享介质被开发出来。该协议发展了各种零碎的机制，用于避免环路、泛洪、地址学习等。相比之下，Fibre Channel的开发人员能够从这些早期的经验中获益，从而创建了一种更全面一致的协议。 </p>
</li>
<li><p>以太网和IP已经发展到可以在各种各样的环境中工作，从局域网到人，从校园到小办公室到家庭。即插即用的向后兼容性对于大多数环境中的应用至关重要;这种现实给协议栈带来了令人畏惧的需求。Fibre Channel仍然专注于高级数据中心用例，因此没有被迫戏剧性地演进，这大大简化了兼容性问题。 </p>
</li>
</ul>
<p>在此适当地承认，iWARP和RoCEv2堆栈的复杂性并不一定会增加明显的延迟; </p>
<p>栈的大部分复杂性是由专门的RDMA enabled NICs或TCP Offload引擎在“硬件”中处理的。但是复杂的栈转换成配置、管理、互操作性、故障排除和分析方面的挑战。 </p>
<h2 id="关于NVMe-oF-InfiniBand"><a href="#关于NVMe-oF-InfiniBand" class="headerlink" title="关于NVMe-oF (InfiniBand)"></a>关于NVMe-oF (InfiniBand)</h2><p>NVMe已成为PCIe ssd的行业标准接口，具有简化的协议和命令集，每个I/O的时钟周期更少。NVMe支持最多64K队列和每个队列最多64K命令，这使得它比现有的基于scsi的协议(如SAS和SATA)更高效。 </p>
<p>NVMe-oF的引入使其更具有可伸缩性，同时仍然得益于低延迟和小开销。NVMexpress.org规范概述了对nvme的支持——NVMe-oF over Ethernet、远程直接内存访问(RDMA)和光纤通道(FC)。 </p>
<p><img src="/pics/image-20210418150744705.png" alt="image-20210418150744705"></p>
<p>相比SCSI, NVMe(NVMe-oF)可以支持更低的延迟I / O，不仅是因为设备速度更快,还因为一些固有的优势在Linux主机操作系统驱动程序栈(如上图)。因此,I / O花费更少的总时间从应用程序的存储,从而减少响应时间。 </p>
<p>市场上的大多数产品的实现是将NVMe驱动器添加到后端存储，前端保持SCSI主机连接，但NetApp E-Series采取了一种单独的方法。从主机到EF570/E5700的前端采用NVMe-oF(InfiniBand)网络，而后端仍然是基于SCSI的SAS驱动器。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/20/store/NVMe/%E5%8D%81%E5%9B%9B%E3%80%81NVMe%20over%20Fabric%E5%B0%86%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98%E5%AD%98%E5%82%A8%E7%8E%AF%E5%A2%83%EF%BC%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/20/store/NVMe/%E5%8D%81%E5%9B%9B%E3%80%81NVMe%20over%20Fabric%E5%B0%86%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98%E5%AD%98%E5%82%A8%E7%8E%AF%E5%A2%83%EF%BC%9F/" class="post-title-link" itemprop="url">十四、NVMe over Fabric将如何改变存储环境？</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-20 23:58:13" itemprop="dateCreated datePublished" datetime="2020-10-20T23:58:13+08:00">2020-10-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>随着光纤通道技术的广泛采用，存储网络在20世纪90年代末和21世纪初开始流行起来。对于那些不想花费安装专用光纤通道硬件的人来说，几年之后，iSCSI协议提供了一种可靠的基于以太网的替代方案。这两种传输都依赖于使用SCSI作为源(启动器)和存储(目标器)之间通信的存储协议。随着存储行业开始采用闪存作为持久存储介质，我们开始看到了SCSI遇到的性能问题。 </p>
<p>这导致了NVMe或非易失性存储器快速发展，NVMe是一种旨在超越SCSI并解决性能问题的新协议。我们来看看NVMe以及它与其他协议的不同之处。在文章中，我们还将探索NVMe如何改变存储网络的格局。 </p>
<h1 id="介质技术遇到瓶颈"><a href="#介质技术遇到瓶颈" class="headerlink" title="介质技术遇到瓶颈"></a>介质技术遇到瓶颈</h1><p>存储网络技术是基于存储硬件的发展和集中存储的需求不断演进的。我们可以将光纤通道的起源追溯到大型机上的ESCON，这是一种基于光纤的连接协议。另一方面，SCSI是基于服务器内硬盘的物理连接。 </p>
<p>SCSI最初是一种并行通信协议——任何熟悉将磁盘安装到服务器中的人都会记得带状电缆。随着SAS的发展，它转变为一个串行接口。与其对应的个人电脑先进主机控制器接口(AHCI)也展成了SATA。或许您可以在当前硬盘驱动器和固态硬盘上找到这两种协议。 </p>
<p>光纤通道或以太网给服务器和存储之间提供了物理连接，SCSI仍然充当高级存储通信协议。然而，业界开发的SCSI与HDD一起工作导致响应时间比系统内存和处理器慢几个数量级。因此，虽然我们可能认为SSD硬盘速度很快，但我们发现内部硬盘存在严重的性能问题。大多数SATA驱动器仍然基于SATA 3.0规范，接口限制为6 Gbps和600 MBps的吞吐量。SAS驱动器已开始转向提供12 GBps吞吐量的SAS 3.0，但许多仍使用6 Gbps连接。 </p>
<p>然而，对SAS和SATA来说，最主要的问题还是单个设备的并发I / O处理能力。看看硬盘的几何形状，很容易看出处理多个并发I/O请求的能</p>
<p>力困难以至于不可能。对性能的提升也是出于偶然性，读/写头或许可以对齐多个请求；或使用一些缓冲，但它不是一个可扩展的选项。SAS和SATA都不是用来处理多个I / O队列的。AHCI有一个深度只有32个命令的单一队列。根据实施情况，SCSI相对好些，可以提供128到256个命令的单个队列。 </p>
<p>单队列但来的负面影响就是延迟。随着队列大小的增加，新请求延迟遇到的延时更大，因为他们必须等待其他请求完成。因为没有机械移动部件，固态硬盘驱动器的问题相对较少，单个I/O延迟较低，但单个队列是固态介质的瓶颈。 </p>
<h1 id="初探NVMe技术"><a href="#初探NVMe技术" class="headerlink" title="初探NVMe技术"></a>初探NVMe技术</h1><p>业界对接口问题的回答是NVMe在设备和网络层面取代SCSI。非易失性存储器使用PCIe总线(而不是专用存储总线)为内部连接的磁盘设备提供更大的带宽和更低的延迟连接。一个PCIe 3.0*4 Lane的设备，具有大约4 GBps的带宽。 </p>
<p>NVMe最大的变化是存储协议的优化。串行化I / O访问所需的内部锁定已经减少，而中断处理的效率已经提高。此外，NVMe最多支持65535个队列，每个队列的队列深度为65535个。因此，NVMe不只有一个队列，还为连接设备的I/O提供了大规模的并行性。想想现代处理器中的内核数量。在并行完成大量工作的IT环境中我们可以看到多个I/O队列并发处理带来的好处，以及这将如何改善外部I / O吞吐量。 </p>
<p>NVM Express工作组是一个由约90家公司组成的联盟，于2012年开发了NVMe规范。该工作组于7月份发布了NVMe规范1.3版，增加了安全性，资源共享和SSD耐久性管理问题的功能。 </p>
<h1 id="什么是NVMe-over-Fabrics"><a href="#什么是NVMe-over-Fabrics" class="headerlink" title="什么是NVMe over Fabrics"></a>什么是NVMe over Fabrics</h1><p>如果NVMe替代设备连接中的存储协议，则不难看出NVMe也可以用iSCSI和光纤通道协议替代SCSI。这正是NVMe over Fabric标准的发展情况，该标准始于2014年，并于去年发布。 </p>
<p>关于NVMe over Fabric，有两种类型的传输正在开发中，使用远程直接内存访问(RDMA)的NVMe over Fabric和使用光纤通道(FC-NVMe)的面向NVMe over Fabric。 </p>
<p>RDMA支持在不涉及处理器的情况下将数据传输到两台计算机的应用程序内存，并提供低延迟和快速数据传输。RDMA实施包括融合以太网上的Infiniband，iWARP和RDMA，或RoCE(发音为“rocky”)。像Mellanox这样的供应商提供适用于Infiniband和以太网的速度可达100 Gbps的适配卡，其中包括NVMe over Fabrics卸载。 </p>
<p>NVMe允许对SSD设备进行大规模并行访问，从而充分利用SSD性能， 3D XPoint的应用更是一个游戏规则改变者的表现。 </p>
<p><img src="/pics/image-20210418144253133.png" alt="image-20210418144253133"></p>
<p>基于光纤通道的NVMe使用当前可升级的光纤通道技术，以支持SCSI和NVMe存储传输。这意味着客户只需通过适当的固件升级交换机即可使用他们现有的技术。在主机系统，主机总线适配器(HBA)必须支持NVMe—通常为16 Gbps或32 Gbps显然，存储设备也必须能够支持NVMe。 </p>
<h1 id="如何实施NVMe"><a href="#如何实施NVMe" class="headerlink" title="如何实施NVMe"></a>如何实施NVMe</h1><p>随着NVMe在数据中心的应用，最明显的选择是在服务器中使用NVMe设备。供应商早已将支持NVMe的服务器推向市场，并提供物理连接器和BIOS支持。大多数现代操作系统已经支持NVMe，就像VMware vSphere等管理程序平台，VMware的vSAN平台早已经支持NVMe设备。 </p>
<p>另一种选择是支持NVMe作为存储设备中的后端存储连接。存储供应商已经开始向SAS过渡作为后端接口，随着时间的推移，取代了光纤通道仲裁环路和并行SCSI。NVMe将取代SAS作为存储阵列的主要内部协议。在优良的架构产品中，这一变化将导致显著的性能改进，因为闪存的好处被发挥了。 </p>
<p>部署NVMe可以为闪存设备提供快速，低延迟的连接，并且在高效存储操作系统代码的情况下显着提高阵列性能。迄今为止，我们已经看到HPE宣布支持3PAR的NVMe，NetApp将NVMe作为FlashCache中的读取缓存引入，Pure Storage将其提供给其FlashArray//X平台等等。 </p>
<p>具有NVMe的Pure Storage的FlashArray//X声称可以提供前一代延迟的一半，并具有两倍的写入带宽。但是，这些规格不包括基于主机的NVMe over Fabrics支持，所以仍然有潜在的性能提升。 </p>
<h1 id="NVMe面对的选择"><a href="#NVMe面对的选择" class="headerlink" title="NVMe面对的选择"></a>NVMe面对的选择</h1><p>NVMe技术的全面采用意味着使用NVMe实现整个SAN服务架构，这正是NVMe over Fabrics所提供的。潜在的客户都采用上述两种实施方案。并有可能转换为适用于基础设施的数据中心的FC-NVMe。思科宣布其高端MDS 9710光纤通道Director支持FC-NVMe。Brocade已经在其Gen6 32 Gbps交换机中支持NVMe，包括最近宣布的G610。 </p>
<p>迁移到NVMe的客户可以避免对光纤通道设备进行翻新和更换。对于已经支持32 Gbps连接的数据中心来说，情况确实如此; 但是，对可能不使用32 Gbps HBA卡服务器来说，情况并非如此。 </p>
<p>支持NVMe的存储阵列出现时，客户可能不必一次升级到NVMe，因为SCSI和NVMe可以共存于同一基础架构上。从管理和运营的角度来看，熟悉光纤通道的数据中心和IT部门可能会发现转型过程比转向融合以太网更容易，因为融合以太网由于更换硬件需要花费而从未真正替换。 </p>
<p>NVMe over Fabrics可以在光纤通道第六代技术及以后的环境共存。这允许将存储阵列转换为支持NVMe的Fabrics，并且比通过以太网迁移到NVMe所需的拆分和替换方法更容易。 </p>
<p>光纤通道的替代方案是将NVMe用于RDMA，并以可扩展性为代价实现新的存储网络，以获得略高的性能。一些供应商使用这种方法提产品。初创公司E8 Storage开发了一种基于NVMe的存储阵列，该阵列使用100 G以太网(GbE)融合交换机和RDMA网络接口卡实施高性能SAN。该公司宣称多达1000万次读取和200万次写入IOPS，读取时间为100微秒(μs)，写入延迟时间为40微秒。 </p>
<p>另一家创业公司Excelero已经开发出一款称为NVMesh的软件产品，该产品使用支持NVMe的服务器网格来创建分布式计算和存储结构，以实现一系列系统，例如超融合计算环境。该公司已与美光公司合作生产基于Micron 3.2 TB固态硬盘和Mellanox以太网RoCE交换机的名为SolidScale的平台。 </p>
<p>该公司宣布支持面向NVMe的NVMe，作为其FlashStack参考架构的一部分。这将包括FlashArray//X，Cisco MDS 9700导向器和思科UCS，或具有32 Gbps HBA的C系列交换机。 </p>
<p>还有一家新兴公司Apeiron Data Systems正在开发基于40 GbE的NVMe阵列体系结构和一种外部化的超融合设计，实现存储和计算能够独立扩展。 </p>
<h1 id="NVMe的未来"><a href="#NVMe的未来" class="headerlink" title="NVMe的未来"></a>NVMe的未来</h1><p>我们将NVMe替代SCSI和SAS作为SSD设备的默认连接。高端部署将使用NVMe over Fabrics，在现有的阵列平台中使用NVMe将会保留了快照，复制，压缩和重复数据删除等功能，并采用了像Excelero和Apeiron这样的新平台体系结构。过去，这些功能的缺乏使得基于NVMe的产品没有受到关注。然而，随着时间的推移，NVMe肯定会取代传统全闪存技术中存在的传统架构。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/19/store/NVMe/%E5%8D%81%E4%B8%89%E3%80%81%E9%80%89%E6%8B%A9%E7%AC%AC%E5%85%AD%E4%BB%A3%E5%85%89%E7%BA%A4%E9%80%9A%E9%81%93%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%89%E4%B8%AA%E5%8E%9F%E5%9B%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/19/store/NVMe/%E5%8D%81%E4%B8%89%E3%80%81%E9%80%89%E6%8B%A9%E7%AC%AC%E5%85%AD%E4%BB%A3%E5%85%89%E7%BA%A4%E9%80%9A%E9%81%93%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%89%E4%B8%AA%E5%8E%9F%E5%9B%A0/" class="post-title-link" itemprop="url">十三、选择第六代光纤通道网络的三个原因</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-19 23:58:13" itemprop="dateCreated datePublished" datetime="2020-10-19T23:58:13+08:00">2020-10-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>升级到下一代光纤通道网络可提供满足不断发展的技术和工作负载需求所需的额外性能，以及其他优势。您是否想知道是否该升级到32 Gbps Gen 6光纤通道网络？开始考虑采取这一举措有三个关键原因： </p>
<p>1、通过不断发展的技术满足您对网络的性能要求 </p>
<p>2、满足更苛刻的工作负载的需求 </p>
<p>3、利用第六代FC提供的众多其他优势 </p>
<p>与16 Gbps Gen 5相比，第6代光纤通道网络使网络吞吐量翻倍。更高的吞吐量可加快应用程序响应时间并消除I / O瓶颈。第6代还引入了并行链接，这使得可以将四个数据通道聚合为一个以创建单个128 Gbps链路。 </p>
<p>迁移到闪存存储的组织可以从第6代功能中获益最多。随着闪存的实施越来越广泛，瓶颈将从存储设备转移到网络。诸如非易失性存储器，NVMe over FC和3D XPoint存储器技术等技术 - 由英特尔和美光联合开发，以填补动态RAM和NAND闪存之间存储市场的空白- 可以帮助消除这些瓶颈并更好地利用闪存的功能。但它们需要足够的网络带宽。 </p>
<p>投资这些较新的闪存和存储器技术的组织必须拥有一个网络，使他们能够充分发挥这些技术的潜力。如果您的光纤通道网络几乎无法满足其当前的数据要求，它肯定无法处理未来的数据要求。最终，您将需要一个能够应对不断增加的工作负载并提供必要性能的网络基础架构。转移到这样一个网络的时间比你意识到的要快。 </p>
<h1 id="不断升级的工作量需求"><a href="#不断升级的工作量需求" class="headerlink" title="不断升级的工作量需求"></a>不断升级的工作量需求</h1><p>不只是闪存存储需要性能更好的网络。今天的工作负载对支持它们的网络提出了前所未有的要求。人工智能，机器学习，大数据分析，虚拟桌面基础架构和数据仓库是推动更大网络需求的应用程序之一。 </p>
<p><img src="/pics/image-20210418143657466.png" alt="image-20210418143657466"></p>
<p>目前使用的这些应用程序越来越多，它们具有比以往更多的特性和功能，并且需要更多的资源。 </p>
<h1 id="超越表现"><a href="#超越表现" class="headerlink" title="超越表现"></a>超越表现</h1><p>性能不是考虑升级到第6代的唯一原因，尽管它仍然是最令人信服的理由。 </p>
<p>性能并不是考虑升级到第6代光纤通道网络的唯一原因，尽管它仍然是最引人注目的网络。您应该考虑的其他好处包括利用Gen 6的前向纠错（FEC）功能。FEC是一种通过发送冗余数据来处理数据传输错误的技术，通过启用误码恢复来提高可靠性。 </p>
<p>第六代光纤通道也比其前代产品更节能。铜线链路可以在不活动时关闭，光纤链路可以使用小睡模式来降低功耗。此外，Gen 6还提供N_Port ID虚拟化技术，该技术定义了多个虚拟服务器如何共享单个物理FC端口标识，以便更轻松地部署服务器虚拟化并扩展SAN结构。 </p>
<p>在做出有关升级到第6代的任何决定之前，您应该执行全面的成本分析，以确定第6代光纤通道网络是否正确。考虑设备和人员的成本以及生产率和客户满意度等因素。这些变量更难以量化，但同样重要，如果不是更重要的话。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/18/store/NVMe/%E5%8D%81%E4%BA%8C%E3%80%81PCIe%20SSD%EF%BC%88PCIe%E5%9B%BA%E6%80%81%E7%A1%AC%E7%9B%98%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="摘星">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摘星">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/18/store/NVMe/%E5%8D%81%E4%BA%8C%E3%80%81PCIe%20SSD%EF%BC%88PCIe%E5%9B%BA%E6%80%81%E7%A1%AC%E7%9B%98%EF%BC%89/" class="post-title-link" itemprop="url">十二、PCIe SSD（PCIe固态硬盘）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-18 23:58:13" itemprop="dateCreated datePublished" datetime="2020-10-18T23:58:13+08:00">2020-10-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Store/" itemprop="url" rel="index"><span itemprop="name">Store</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PCIe SSD（PCIe 固态驱动器）是一种高速扩展卡，可将计算机连接到其外围设备。PCIe或Peripheral Component Interconnect Express是串行扩展总线标准。PCIe 插槽可以根据连接到它的双向通道的数量而具有不同的大小。PCIe SSD 存储是服务器端闪存部署的替代方案。 </p>
<p>PCIe SSD 直接在服务器主板上集成闪存。每个PCIe 设备通过其自己的串行链路连接到主机，无需共享总线。点对点架构可降低延迟并提高服务器和存储阵列之间的数据传输速率。 </p>
<p>PCIe SSD是常见四种SSD 外形之一。通常，基于PCIe的固态存储比基于服务器的串行ATA（SATA），串行连接SCSI（SAS）或光纤通道SSD 具有更好的性能。 </p>
<p>基于PCIe的设备的格式规范由PCI特别兴趣小组（PCI-SIG）开发和维护。PCIe 3.0于2010年11月发布。当前版本PCIe 4.0在2017年发布。 </p>
<p><img src="/pics/image-20210418143132914.png" alt="image-20210418143132914"></p>
<p>针对非易失性存储器（NVMe）规范设计的PCIe SSD正在支持。NVMe协议提供用于访问PCIe SSD的优化命令集。NVMe利用PCIe 3.0的并行性来加速性能。 </p>
<p>PCI-SIG还为M.2固态硬盘制定了标准，这是一种专为内置扩展卡设计的小巧外形。M.2规范取代了mSATA规格。M.2 SSD配备四个PCIe 3.0 带宽通道。 </p>
<h1 id="PCIe-SSD与SATA-SSD"><a href="#PCIe-SSD与SATA-SSD" class="headerlink" title="PCIe SSD与SATA SSD"></a>PCIe SSD与SATA SSD</h1><p>如前所述，PCIe连接的SSD可避免与SATA或SAS连接的SSD相关的瓶颈。每个SSD的PCIe通道数决定了数据传输的速度。基于PCIe 3.0规范构建的16通道设备每秒可支持大约32 千兆字节。相比之下，使用SATA III控制器构建的SSD 提供的最大传输速率约为每秒600 兆字节。 </p>
<p>PCIe驱动器供应商仅逐步实施3.0规范。大多数供应商仍在发布基于PCIe 2.0的产品。 </p>
<p>SATA v3.2规范定义了SATA Express连接器，用于同时支持SATA和PCIe 协议的主机和设备连接器。 </p>
<h1 id="PCIe-SSD的缺点"><a href="#PCIe-SSD的缺点" class="headerlink" title="PCIe SSD的缺点"></a>PCIe SSD的缺点</h1><p>PCIe多用途总线将各种数据传送到处理器。尽管PCIe固有的固有性能优势，但其每GB的成本比传统固态硬盘更高。 </p>
<p>缺少标准存储命令是另一个缺点。PCIe SSD设备制造商都需要编写和定制相应操作系统的软件驱动程序。 </p>
<h1 id="PCIe闪存的未来"><a href="#PCIe闪存的未来" class="headerlink" title="PCIe闪存的未来"></a>PCIe闪存的未来</h1><p>在可预见的未来，PCIe闪存将在企业数据中心与传统SSD和非易失性存储器技术共存。据行业分析师称，采用基于闪存的双列直插式内存模块卡的非易失性存储可能会在企业采用中挑战PCIe闪存。 </p>
<p>了解PCIe SSD的优缺点，NAND闪存的工作原理，PCIe SSD的典型使用案例，以及该领域的供应商和产品。有助于全面学习PCIe相关知识。 </p>
<p>PCI Express或PCIe是计算机主板互连，用于扩展服务器的性能，功能或连接。HBA，NIC，图形加速器卡和RAM扩展模块是常见的例子。NAND闪存形式的固态存储设备是PCIe互连的新应用。下面将详细介绍PCIe SSD：以下主要介绍板载SSD的PCIe卡(为服务器提供内部存储)，而不是通过PCIe接口卡连接的外部 SSD阵列。 </p>
<p>基于闪存的SSD的使用越来越多，因为磁盘存储无法跟上CPU性能; 硬盘转速在近十年内没有增加。尽管一些性能改进来自数据密度的增加（每次轮换的数据更多），但事实仍然是基于硬盘的存储无法足够快地从CPU获取数据以利用不断改进的处理器。内存（DRAM）太昂贵，无法提供所需的容量，因此已经开发了闪存存储来填补这一空白。闪存的性能比硬盘驱动器更接近DRAM，但每两千兆字节的成本介于两者之间，闪存为旋转磁盘存储提供了一种可行的替代方案。 </p>
<p>PCIe提供了一个高效的平台，可将此闪存集成到计算环境中，将其放在服务器主板上，并将其（物理上和逻辑上）移近服务器计算引擎。与外部设备（例如具有SSD的传统磁盘阵列而不是硬盘驱动器，专用SSD阵列或缓存设备）相比，PCIe卡存储上的闪存可以绕过直接连接的外部设备所具有的阵列控制器，接口协议和布线。它们还消除了SAN连接阵列的交换机和网络连接。 </p>
<h1 id="NAND闪存如何工作"><a href="#NAND闪存如何工作" class="headerlink" title="NAND闪存如何工作"></a>NAND闪存如何工作</h1><p>作为数据记录介质，NAND闪存具有许多需要特殊处理的特性。这些过程表示由驻留在PCIe卡上的闪存控制器或服务器CPU执行的开销。通常，它们与闪存写入操作相关联，但也处理可靠性过程，如纠错和更换坏闪存块。 </p>
<p>当数据写入闪存时，必须在每次写入之前擦除旧数据，以便为新数据腾出空间。此外，此闪存擦除必须以块的形式发生，而不是像HDD那样在字节级发生。这意味着当删除一部分数据时，必须擦除它所在的整个块，并且必须将未标记为删除的字节复制到另一个位置。 </p>
<p>与磁盘基板不同，NAND闪存具有有限的寿命，这意味着它只能被写入并擦除一定次数。响应这种情况，闪存控制器在所有可用块上扩展数据写入，以便整个芯片以更均匀的速率老化。这种称为“磨损均衡”的工艺可以最大限度地延长NAND基板的使用寿命，从而最大限度地延长NAND基板的使用寿命。 </p>
<p>PCIe卡上的闪存使SSD最接近服务器内的CPU内存复合体，并提供比外部SSD设备更低的潜在延迟。PCIe是双向的，这意味着读取和写入可以同时发生，并且它消除了SAS，SATA或FC存储的协议转换。所有这些加起来比使用这些协议的外部设备更快的I / O，并且通常还必须与RAID功能和存储功能共享CPU。显然，网络连接的闪存存储系统将额外的网络延迟放入等式中。 </p>
<p>由于它是专用的专用存储设备，因此PCIe SSD卡最易于安装和使用。外部系统，尤其是连接到存储网络并与其他主机共享的系统，通常实现起来更复杂。这些大容量存储设备代表了更大的投资，并且通常涉及存储分层和缓存等策略，以提高SSD利用率。 </p>
<p>作为内部总线连接卡，PCIe SSD设备在安装或更换时会导致重新启动，从而使扩展或服务更具破坏性。此外，由于它们使用PCIe插槽，因此可能会减少其他PCIe设备的数量，例如可以使用的网络适配器（或其他SSD卡）。作为专用存储，PCIe SSD无法与其他服务器共享，这有助于降低这种高价容量的成本。 </p>
<h1 id="PCIe-SSD用例和产品"><a href="#PCIe-SSD用例和产品" class="headerlink" title="PCIe SSD用例和产品"></a>PCIe SSD用例和产品</h1><p>PCIe SSD是I / O密集型应用程序的理想选择，如Web 2.0，社交网络，OLTP，活动数据库等。它还可以为需要大型，快速缓冲区域的本地化应用程序问题提供快速性能改进，例如视频编辑，财务建模，模拟和数据采集。与外部或网络连接阵列的潜在复杂性相比，这种直连存储（DAS）实施也可能吸引初次使用SSD的用户。这种外形尺寸也非常适合OEM将SSD和服务器作为升级选项。 </p>
<p>Texas Memory Systems将闪存控制器放在PCIe卡上，因此它不会影响服务器CPU或内存资源来处理前面提到的写入开销。根据营销总监Levi Norman的说法，“NAND闪存返回错误;您只需管理它.Dexory Memory Systems将PCIe卡设计为系统，具有板载RAM和CPU，使其具有执行更高效I / O的智能和提高性能，特别是对于大量使用的应用程序。“ </p>
<p>Fusion-io技术营销高级主管Mitch Crane表示，“其他人都在努力让闪存看起来像磁盘驱动器;我们让它看起来就像是内存。” 他说Fusion-io的产品使用物理内存地址和DMA来处理闪存更像系统内存而不是存储。Fusion-io产品不会在卡上执行闪存开销功能，Crane称这会增加复杂性和失败的可能性。 </p>
<p>LSI的PCIe SSD解决方案采用了一种不同的方法。它的卡包括六个SSD驱动器模块，以及作为PCIe和这些模块之间接口的SAS / SATA桥接器。闪存控制器功能由SSD模块本身处理。该公司的主要产品经理JB Baker表示，“在解决方案架构中使用SAS控制器可以使LSI在未来的设计中具有灵活性，可以考虑将端口连接到外部SAS / SATA设备，从而使用户能够扩展和扩展系统。 “ </p>
<h1 id="关于PCIe的SSD产品建议"><a href="#关于PCIe的SSD产品建议" class="headerlink" title="关于PCIe的SSD产品建议"></a>关于PCIe的SSD产品建议</h1><p>虽然基于PCIe的SSD产品较新，并且目前占SSD总市场的一小部分，但它们肯定是一个热门话题。据Gartner称，基于PCIe的固态硬盘的市场份额在2009年占固态出货量的6％，预计到2013年将是所有固态硬盘供应商推出产品的四倍。从性能角度来看，PCIe SSD代表了一种快速有效的方法，可将闪存性能应用于服务器IOPS问题。虽然性能是SSD的关键驱动因素，但实际里程可能会因所选PCIe解决方案和环境而异。但性能是集成商习惯与客户讨论的讨论。另一个是实施。 </p>
<p>从业人员发现，从他们的SSD投资中获得他们需要的那种利用率说起来容易做起来难。将SSD添加到现有阵列或放入专用SSD设备可能是一个复杂而昂贵的解决</p>
<p>方案。作为SSD的较小的本地化实现，PCIe卡可以更容易获得批准，更容易投入并更快地显示预期的性能改进。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="摘星"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">摘星</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">261</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">142</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangkexuan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangkexuan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/kexuan_zhang@qq.com" title="E-Mail → kexuan_zhang@qq.com"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">摘星</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>

<!-- ҳ����С���� -->
<script type="text/javascript" src="/js/clicklove.js"></script>
